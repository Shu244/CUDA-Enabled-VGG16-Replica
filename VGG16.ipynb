{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG16.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shu244/CUDA-Enabled-VGG16-Replica/blob/master/VGG16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRr5tIk6rMZF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Sets up environment variables to allow cudatoolkit and numba to function properly.\n",
        "import os\n",
        "os.environ['NUMBAPRO_LIBDEVICE'] = \"/usr/local/cuda-10.0/nvvm/libdevice\"\n",
        "os.environ['NUMBAPRO_NVVM'] = \"/usr/local/cuda-10.0/nvvm/lib64/libnvvm.so\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyRK7AOd6uKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oHdbqWCrRmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib.image import imread\n",
        "import numpy as np\n",
        "from numba import cuda, vectorize\n",
        "import h5py\n",
        "\n",
        "\n",
        "@cuda.jit\n",
        "def matrix_mul(matrix1, matrix2, result):\n",
        "    '''\n",
        "    Matrix 1 and 2 must be 2D arrays.\n",
        "    '''\n",
        "\n",
        "    thread_r, thread_c = cuda.grid(2)  # Gets position of thread.\n",
        "    thread_r_size, thread_c_size = cuda.gridsize(2)  # Gets total number of threads in each dimension.\n",
        "\n",
        "    result_r, result_c = result.shape\n",
        "\n",
        "    for thread_r_i in range(thread_r, result_r, thread_r_size):\n",
        "        for thread_c_i in range(thread_c, result_c, thread_c_size):\n",
        "            # The following code will calculate one element in the result matrix at location thread_r_i and thread_c_i.\n",
        "            for vector_i in range(0, matrix2.shape[0]):\n",
        "                result[thread_r_i, thread_c_i] = result[thread_r_i, thread_c_i] + matrix1[thread_r_i, vector_i] * matrix2[vector_i, thread_c_i]\n",
        "\n",
        "\n",
        "@cuda.jit\n",
        "def same_convolve_multiple_filters(unpadded_input, kernal, result):\n",
        "    '''\n",
        "    Strides = 1.\n",
        "    unpadded_input and result must have the same shape.\n",
        "    Expected shape of the unpadded_input: channels, height, width\n",
        "    Expected shape of the kernal: kernals, channels, height, width.\n",
        "    Expected shape result: channels, height, width\n",
        "    '''\n",
        "    thread_channel, thread_r, thread_c = cuda.grid(3)  # Gets position of thread.\n",
        "    thread_channel_size, thread_r_size, thread_c_size = cuda.gridsize(\n",
        "        3)  # Gets total number of threads in each dimension.\n",
        "\n",
        "    kernal_channels, kernal_r, kernal_c = kernal.shape[1:]\n",
        "\n",
        "    # Padding that must be applied to all borders to maintain same dimension.\n",
        "    r_pad, c_pad = ((kernal_r - 1) // 2), ((kernal_c - 1) // 2)\n",
        "\n",
        "    result_channel, result_r, result_c = result.shape\n",
        "\n",
        "    for thread_channel_i in range(thread_channel, result_channel, thread_channel_size):\n",
        "        for thread_r_i in range(thread_r, result_r, thread_r_size):\n",
        "            for thread_c_i in range(thread_c, result_c, thread_c_size):\n",
        "                corner_input_r_i = thread_r_i - r_pad\n",
        "                corner_input_c_i = thread_c_i - c_pad\n",
        "                for kernal_channel_i in range(0, kernal_channels):\n",
        "                    for kernal_r_i in range(0, kernal_r):\n",
        "                        for kernal_c_i in range(0, kernal_c):\n",
        "                            input_r_i = corner_input_r_i + kernal_r_i\n",
        "                            input_c_i = corner_input_c_i + kernal_c_i\n",
        "                            if 0 <= input_r_i < result_r and 0 <= input_c_i < result_c:\n",
        "                                new_result = unpadded_input[kernal_channel_i, input_r_i, input_c_i] * kernal[thread_channel_i, kernal_channel_i, kernal_r_i, kernal_c_i]\n",
        "                                result[thread_channel_i, thread_r_i, thread_c_i] = result[thread_channel_i, thread_r_i, thread_c_i] + new_result\n",
        "\n",
        "\n",
        "@cuda.jit\n",
        "def max_pooling_multiple_filters(input, window, result):\n",
        "    '''\n",
        "    Expectation: the dimensions of the input is disivible by dimensions of window. Further, windows do not overlap.\n",
        "    Expected shape of the input: channels, height, width\n",
        "    \"window\" is a tuple of format (height, width)\n",
        "    Expected shape of result: channels, height, width\n",
        "    '''\n",
        "    thread_channel, thread_r, thread_c = cuda.grid(3)  # Gets position of thread.\n",
        "    thread_channel_size, thread_r_size, thread_c_size = cuda.gridsize(3)  # Gets total number of threads in each dimension.\n",
        "\n",
        "    window_r, window_c = window\n",
        "\n",
        "    result_channel, result_r, result_c = result.shape\n",
        "\n",
        "    for thread_channel_i in range(thread_channel, result_channel, thread_channel_size):\n",
        "        for thread_r_i in range(thread_r, result_r, thread_r_size):\n",
        "            for thread_c_i in range(thread_c, result_c, thread_c_size):\n",
        "                input_r_corner_i = thread_r_i * window_r\n",
        "                input_c_corner_i = thread_c_i * window_c\n",
        "                max = input[thread_channel_i, input_r_corner_i, input_c_corner_i]\n",
        "                for window_r_i in range(0, window_r):\n",
        "                    for window_c_i in range(0, window_c):\n",
        "                        element = input[thread_channel_i, input_r_corner_i + window_r_i, input_c_corner_i + window_c_i]\n",
        "                        if element > max:\n",
        "                            max = element\n",
        "                result[thread_channel_i, thread_r_i, thread_c_i] = max\n",
        "\n",
        "\n",
        "@cuda.jit\n",
        "def add_biases_2D(input, biases):\n",
        "    '''\n",
        "    Input is of format: height, width\n",
        "    There must be one bias per input element.\n",
        "    is one bias per layer.\n",
        "    '''\n",
        "    thread_r, thread_c = cuda.grid(2)  # Gets position of thread.\n",
        "    thread_r_size, thread_c_size = cuda.gridsize(2)  # Gets total number of threads in each dimension.\n",
        "\n",
        "    result_r, result_c = input.shape\n",
        "\n",
        "    for thread_r_i in range(thread_r, result_r, thread_r_size):\n",
        "        for thread_c_i in range(thread_c, result_c, thread_c_size):\n",
        "            input[thread_r_i, thread_c_i] = input[thread_r_i, thread_c_i] + biases[thread_r_i, thread_c_i]\n",
        "\n",
        "\n",
        "@cuda.jit\n",
        "def add_biases_3D(input, biases):\n",
        "    '''\n",
        "    Input is of format: channels, height, width\n",
        "    The number of biases must be equal to the number of layers as there\n",
        "    is one bias per layer.\n",
        "    '''\n",
        "    thread_channel, thread_r, thread_c = cuda.grid(3)  # Gets position of thread.\n",
        "    thread_channel_size, thread_r_size, thread_c_size = cuda.gridsize(3)  # Gets total number of threads in each dimension.\n",
        "\n",
        "    result_channel, result_r, result_c = input.shape\n",
        "\n",
        "    for thread_channel_i in range(thread_channel, result_channel, thread_channel_size):\n",
        "        for thread_r_i in range(thread_r, result_r, thread_r_size):\n",
        "            for thread_c_i in range(thread_c, result_c, thread_c_size):\n",
        "                input[thread_channel_i, thread_r_i, thread_c_i] = input[thread_channel_i, thread_r_i, thread_c_i] + biases[thread_channel_i]\n",
        "\n",
        "\n",
        "# Using float64 because h5 file stores parameters in float64.\n",
        "@vectorize(['float32(float32)', 'float64(float64)'], target='cuda')\n",
        "def ReLU(activation):\n",
        "    if activation > 0:\n",
        "        return activation\n",
        "    return 0\n",
        "    ''' \n",
        "    return max(activation, 0) causes racing condition to occur. \n",
        "    '''\n",
        "\n",
        "\n",
        "def softmax(activations):\n",
        "    '''\n",
        "    Used only in last layer with 1000 neurons. Not worth the data transfer speed\n",
        "    to involve GPU.\n",
        "    '''\n",
        "    eactivations = np.exp(activations)\n",
        "    etotal = sum(eactivations)\n",
        "    return eactivations / etotal\n",
        "\n",
        "\n",
        "class VGG16:\n",
        "    def __init__(self, params_path):\n",
        "        # reading in weights and baises now.\n",
        "        self.params = h5py.File(params_path, 'r')\n",
        "        self.input = None\n",
        "\n",
        "    def classify(self, image_path):\n",
        "        img = imread(image_path)\n",
        "        # Flips image from RGB to BGR\n",
        "        img = np.flip(img, [2])\n",
        "        # Gets image to be in the format: channels, height, width\n",
        "        img = np.rollaxis(img, 2, 0)  \n",
        "        print('Tranposed original image:\\n\\n',img[0,:5,:5])\n",
        "        # Put img array in C-contiguous format\n",
        "        img = img.copy(order='C')\n",
        "        img_device = cuda.to_device(img)\n",
        "        self.input = img_device\n",
        "        return self.forward_propagation()\n",
        "\n",
        "    def forward_propagation(self):\n",
        "        layers_with_pool = (1, 3, 6, 9, 12)\n",
        "        threads_per_block = (4, 8, 16)\n",
        "        blocks_per_grid = (10, 10)\n",
        "        pool_window_height, pool_window_width = 2, 2\n",
        "        \n",
        "#         #TEST\n",
        "#         orinput = self.input.copy_to_host()\n",
        "#         print('original image:\\n\\n',orinput[0,:5,:5])\n",
        "\n",
        "        # going through all convolution layers.\n",
        "        for i in range(0, 13):\n",
        "            filters = self.params[str(i)]['weights']    \n",
        "            # Eventually remove this step by saving the data properly. This flips weights so it is in proper format\n",
        "            filters = np.array(filters)\n",
        "            filters= filters[:,:,::-1,::-1]\n",
        "            filters = np.ascontiguousarray(filters)      \n",
        "            biases = self.params[str(i)]['biases']\n",
        "            filters_device = cuda.to_device(filters)\n",
        "            biases_device = cuda.to_device(biases)\n",
        "\n",
        "            # Computes convolution.\n",
        "            result = np.zeros((filters.shape[0],) + self.input.shape[1:])\n",
        "            out_device = cuda.device_array_like(result)\n",
        "            same_convolve_multiple_filters[blocks_per_grid, threads_per_block](self.input, filters_device, out_device)\n",
        "\n",
        "            # Applying biases now.\n",
        "            add_biases_3D[blocks_per_grid, threads_per_block](out_device, biases_device)          \n",
        "\n",
        "            # Applying ReLU now.\n",
        "            out_device = ReLU(out_device)           \n",
        "            \n",
        "#             # Test print\n",
        "#             test = out_device.copy_to_host()\n",
        "#             print('weights:\\n\\n', filters[0,2,:,:])\n",
        "#             print(\"outputs: \\n\\n\", test[0, :10, :10])\n",
        "#             return\n",
        "\n",
        "            # Applying maxpool now, if necessary.\n",
        "            if i in layers_with_pool:\n",
        "                channels, height, width = out_device.shape\n",
        "                window_device = cuda.to_device((pool_window_height, pool_window_width))\n",
        "                new_out_device = cuda.device_array((channels, height // pool_window_height, width // pool_window_width))\n",
        "                max_pooling_multiple_filters[blocks_per_grid, threads_per_block](out_device, window_device, new_out_device)\n",
        "                out_device = new_out_device\n",
        "\n",
        "            self.input = out_device\n",
        "\n",
        "        # Must apply fully connected layers now.\n",
        "        threads_per_block = (32, 16)\n",
        "        blocks_per_grid = (10, 10)\n",
        "        # Converting volume to vector.\n",
        "        num_elements = np.prod(self.input.shape)\n",
        "        self.input = self.input.reshape((1, num_elements))\n",
        "        for i in range(13, 16):\n",
        "            # Setting up for CUDA Numba.\n",
        "            weights = self.params[str(i)]['weights']\n",
        "            biases = self.params[str(i)]['biases']\n",
        "            weights_device = cuda.to_device(weights)\n",
        "            biases_device = cuda.to_device(biases)\n",
        "\n",
        "            mult_result = np.zeros((self.input.shape[0], weights.shape[1]))\n",
        "            device_mult_result = cuda.to_device(mult_result)\n",
        "\n",
        "            # Performing matrix multiplication on GPU.\n",
        "            matrix_mul[blocks_per_grid, threads_per_block](self.input, weights_device, device_mult_result)\n",
        "\n",
        "            # Must add biases now.\n",
        "            biases_device = biases_device.reshape(device_mult_result.shape)\n",
        "            add_biases_2D(device_mult_result, biases_device)          \n",
        "\n",
        "            # Must apply ReLU now\n",
        "            self.input = ReLU(device_mult_result)\n",
        "\n",
        "        # Now applying softmax functon.\n",
        "        result = self.input.copy_to_host()\n",
        "        result = result.reshape(result.shape[1])\n",
        "        return softmax(result)\n",
        "    \n",
        "    def save_weights(path):\n",
        "        hf = h5py.File(path, 'w')\n",
        "        hf.close()\n",
        "    \n",
        "    def run_one_epoch(dataset, batch_size):\n",
        "        start = 0\n",
        "        end = batch_size\n",
        "        L = len(dataset)\n",
        "        while start <= L:     \n",
        "            mini_batch = dataset[start:end]\n",
        "            \n",
        "            for one_data in mini_batch:\n",
        "                image_path, label = one_data\n",
        "                logits = self.classifies(image_path)\n",
        "                \n",
        "            \n",
        "            start = start + batch_size\n",
        "            end = end + bathc_size\n",
        "    \n",
        "    def train(dataset, epochs, batch_size):\n",
        "        '''\n",
        "        data set is array of tuple = (feature, label)\n",
        "        '''\n",
        "        dataset = np.random.shuffle(dataset)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duJT6ySi0wRm",
        "colab_type": "code",
        "outputId": "97f6a454-7644-461d-d7ab-9485b9c44e55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        }
      },
      "source": [
        "image_path1 = '/content/drive/My Drive/Colab Notebooks/VGG16 Test Data/dog photo.jpg'\n",
        "image_path2 = '/content/drive/My Drive/Colab Notebooks/VGG16 Test Data/orangutan2.jpg'\n",
        "image_path3 = '/content/drive/My Drive/Colab Notebooks/VGG16 Test Data/orangutan3.jpg'\n",
        "image_path4 = '/content/drive/My Drive/Colab Notebooks/VGG16 Test Data/orangutan4.jpg'\n",
        "image_path5 = '/content/drive/My Drive/Colab Notebooks/VGG16 Test Data/orangutan5.jpg'\n",
        "image_path6 = '/content/drive/My Drive/Colab Notebooks/VGG16 Test Data/orangutan6.jpg'\n",
        "data_path = '/content/drive/My Drive/Colab Notebooks/VGG16 Test Data/vgg16_weights_reformatted.h5'\n",
        "\n",
        "# #Test\n",
        "# from PIL import Image\n",
        "# img = Image.open(image_path1)\n",
        "# img = np.flip(img, [2])\n",
        "# img = np.expand_dims(img, axis=0)\n",
        "# print(\"Just read Image:\\n\\n\", img[0,:5,:5,0])\n",
        "\n",
        "model = VGG16(data_path)\n",
        "print(np.argmax(model.classify(image_path1)))\n",
        "# print(model.classify(image_path2).argmax())\n",
        "# print(model.classify(image_path3).argmax())\n",
        "# print(model.classify(image_path4).argmax())\n",
        "# print(model.classify(image_path5).argmax())\n",
        "# print(model.classify(image_path6).argmax())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tranposed original image:\n",
            "\n",
            " [[145 145 146 146 147]\n",
            " [144 145 145 146 147]\n",
            " [144 144 144 145 146]\n",
            " [143 143 144 144 145]\n",
            " [141 141 142 142 143]]\n",
            "original image:\n",
            "\n",
            " [[145 145 146 146 147]\n",
            " [144 145 145 146 147]\n",
            " [144 144 144 145 146]\n",
            " [143 143 144 144 145]\n",
            " [141 141 142 142 143]]\n",
            "weights:\n",
            "\n",
            " [[ 0.4800154   0.4085474  -0.06514555]\n",
            " [ 0.31047726  0.05020237 -0.40338343]\n",
            " [-0.05087169 -0.2852275  -0.41851634]]\n",
            "outputs: \n",
            "\n",
            " [[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.        ]\n",
            " [ 0.          3.67474568  3.35007679  1.37565672  1.15158713  2.49530613\n",
            "   2.45618212  6.6246382   6.22197354  2.53823292]\n",
            " [ 0.          4.56247675  3.71220553  2.21119511  2.59774458  3.86510336\n",
            "   3.37345707  5.51669276  4.92909062  2.53823292]\n",
            " [ 0.          4.70175135  3.74522173  3.04567969  2.29337418  3.73705447\n",
            "   3.65078461  3.7896663   4.08028138  2.45343316]\n",
            " [ 0.          4.49325573  4.96670783  4.50492871  2.29274046  2.51233923\n",
            "   3.77968132  2.0343796   1.86746991  2.46450055]\n",
            " [ 0.          4.56340277  3.78322709  2.44329655  1.31776726  2.20760882\n",
            "   5.0719229   0.87874949  0.          2.54634011]\n",
            " [ 0.          2.08332503  1.33267939  1.00044644  0.94335806  0.82551682\n",
            "   2.59185994  0.          0.          2.3767401 ]\n",
            " [ 0.         10.08010495  9.97017682 10.01933777  9.30794346  9.3511802\n",
            "  10.21314585  2.41527712  1.53688157  7.16338456]\n",
            " [ 0.          7.69177783  6.26914227  4.29472268  4.10018504  6.21414053\n",
            "   7.72481787  0.          1.1229502   7.60144484]\n",
            " [ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          6.07569706]]\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYVTIIblPfy5",
        "colab_type": "code",
        "outputId": "702d81da-b92a-48a7-9f2f-a433208904d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "a = np.array([[1,2,3],[4,5,6],[7,8,9],[11,12,13]])\n",
        "b = np.flip(a, (1))\n",
        "print(b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 3  2  1]\n",
            " [ 6  5  4]\n",
            " [ 9  8  7]\n",
            " [13 12 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlCM0Gg5DZTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_propagation(image_path):\n",
        "    img = imread(image_path)\n",
        "    # Gets image to be in the format: channels, height, width\n",
        "    img = np.transpose(img, (2, 1, 0))\n",
        "    # Put img array in C-contiguous format\n",
        "    img = img.copy(order='C')\n",
        "    img_device = cuda.to_device(img)\n",
        "    input = img_device\n",
        "    \n",
        "    layers_with_pool = (1, 3, 6, 9, 12)\n",
        "    threads_per_block = (4, 8, 16)\n",
        "    blocks_per_grid = (10, 10)\n",
        "    pool_window_height, pool_window_width = 2, 2\n",
        "    \n",
        "    # layers, num filters, channels, height, width\n",
        "    all_filters = np.array([\n",
        "        [\n",
        "            [\n",
        "                [\n",
        "                    [0.1, -0.1, 0.2],\n",
        "                    [0.3, 0.2, 0.1],\n",
        "                    [-0.1, -0.1, -0.1]\n",
        "                ],\n",
        "                [\n",
        "                    [0.2, -0.1, 0.2],\n",
        "                    [0.1, 0.3, 0.1],\n",
        "                    [-0.2, -0.1, -0.1]\n",
        "                ],\n",
        "                [\n",
        "                    [0.1, -0.1, 0.2],\n",
        "                    [0.3, 0.2, 0.1],\n",
        "                    [-0.1, -0.1, -0.1]\n",
        "                ]\n",
        "            ],\n",
        "            [             \n",
        "                [\n",
        "                    [0.2, -0.1, 0.2],\n",
        "                    [0.1, 0.3, 0.1],\n",
        "                    [-0.2, -0.1, -0.1]\n",
        "                ],\n",
        "                [\n",
        "                    [0.1, -0.1, 0.2],\n",
        "                    [0.3, 0.2, 0.1],\n",
        "                    [-0.1, -0.1, -0.1]\n",
        "                ],\n",
        "                [\n",
        "                    [0.1, -0.1, 0.2],\n",
        "                    [0.3, 0.2, 0.1],\n",
        "                    [-0.1, -0.1, -0.1]\n",
        "                ]\n",
        "            ],\n",
        "            [\n",
        "                [\n",
        "                    [0.1, -0.1, 0.2],\n",
        "                    [0.3, 0.2, 0.1],\n",
        "                    [-0.1, -0.1, -0.1]\n",
        "                ],                \n",
        "                [\n",
        "                    [0.1, -0.1, 0.2],\n",
        "                    [0.3, 0.2, 0.1],\n",
        "                    [-0.1, -0.1, -0.1]\n",
        "                ],\n",
        "                [\n",
        "                    [0.2, -0.1, 0.2],\n",
        "                    [0.1, 0.3, 0.1],\n",
        "                    [-0.2, -0.1, -0.1]\n",
        "                ]\n",
        "            ]\n",
        "        ],\n",
        "        [\n",
        "            [\n",
        "                [\n",
        "                    [0.2, -0.1, 0.2],\n",
        "                    [0.1, 0.5, 0.1],\n",
        "                    [-0.1, -0.1, -0.1]\n",
        "                ],\n",
        "                [\n",
        "                    [0.1, -0.1, 0.2],\n",
        "                    [0.3, 0.2, 0.1],\n",
        "                    [-0.1, -0.1, -0.1]\n",
        "                ],\n",
        "                [\n",
        "                    [0.3, -0.1, 0.2],\n",
        "                    [0.3, 0.4, 0.1],\n",
        "                    [-0.2, -0.1, -0.1]\n",
        "                ]\n",
        "            ],\n",
        "            [                \n",
        "                [\n",
        "                    [0.1, -0.1, 0.2],\n",
        "                    [0.3, 0.2, 0.1],\n",
        "                    [-0.1, -0.1, -0.1]\n",
        "                ],\n",
        "                [\n",
        "                    [0.2, -0.1, 0.2],\n",
        "                    [0.1, 0.5, 0.1],\n",
        "                    [-0.1, -0.1, -0.1]\n",
        "                ],\n",
        "                [\n",
        "                    [0.3, -0.1, 0.2],\n",
        "                    [0.3, 0.4, 0.1],\n",
        "                    [-0.2, -0.1, -0.1]\n",
        "                ]\n",
        "            ],\n",
        "            [               \n",
        "                [\n",
        "                    [0.1, -0.1, 0.2],\n",
        "                    [0.3, 0.2, 0.1],\n",
        "                    [-0.1, -0.1, -0.1]\n",
        "                ],\n",
        "                [\n",
        "                    [0.3, -0.1, 0.2],\n",
        "                    [0.3, 0.4, 0.1],\n",
        "                    [-0.2, -0.1, -0.1]\n",
        "                ],\n",
        "                [\n",
        "                    [0.2, -0.1, 0.2],\n",
        "                    [0.1, 0.5, 0.1],\n",
        "                    [-0.1, -0.1, -0.1]\n",
        "                ]\n",
        "            ]\n",
        "        ]\n",
        "    ])\n",
        "    all_biases = np.array([\n",
        "        [\n",
        "            1, 2, 3\n",
        "        ],\n",
        "        [\n",
        "            4, 5, 6\n",
        "        ]\n",
        "    ])\n",
        "\n",
        "    # going through all convolution layers.\n",
        "    for i in range(0, 13):\n",
        "        filters = all_filters[i]\n",
        "        biases = all_biases[i]\n",
        "        filters_device = cuda.to_device(filters)\n",
        "        biases_device = cuda.to_device(biases)\n",
        "\n",
        "        # Printing out sample of inputs for debugging\n",
        "        test_input = input.copy_to_host()\n",
        "        print('Layer ' + str(i) + ' input:\\n\\n', test_input[:, :4, :4], '\\n\\n')\n",
        "        print('weights: \\n\\n', filters[:, :3, :4, :4], '\\n\\n')\n",
        "        print('biases: \\n\\n', biases[:3], '\\n\\n')\n",
        "\n",
        "        # Computes convolution.\n",
        "        result = np.zeros((filters.shape[0],) + input.shape[1:])\n",
        "        out_device = cuda.device_array_like(result)\n",
        "        same_convolve_multiple_filters[blocks_per_grid, threads_per_block](input, filters_device, out_device)\n",
        "\n",
        "        out_device_test = out_device.copy_to_host()\n",
        "        print('output:\\n\\n', out_device_test[:, :3, :3], '\\n\\n')\n",
        "        \n",
        "        # Applying biases now.\n",
        "        add_biases_3D[blocks_per_grid, threads_per_block](out_device, biases_device)\n",
        "\n",
        "        # Applying ReLU now.\n",
        "        out_device = ReLU(out_device)\n",
        "\n",
        "        # Applying maxpool now, if necessary.\n",
        "        if i in layers_with_pool:\n",
        "            # Printing out input to maxpool \n",
        "            test_test = out_device.copy_to_host()\n",
        "            print('input for maxpool: \\n\\n', test_test[:, :2, :2])\n",
        "            \n",
        "            channels, height, width = out_device.shape\n",
        "            window_device = cuda.to_device((pool_window_height, pool_window_width))\n",
        "            new_out_device = cuda.device_array((channels, height // pool_window_height, width // pool_window_width))\n",
        "            max_pooling_multiple_filters[blocks_per_grid, threads_per_block](out_device, window_device, new_out_device)\n",
        "            out_device = new_out_device\n",
        "\n",
        "            # Printing out sample of inputs for debugging\n",
        "            test_out = out_device.copy_to_host()\n",
        "            print('Maxpool: \\n\\n', test_out[:, 0, 0], '\\n\\n')\n",
        "            return None\n",
        "\n",
        "        input = out_device\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M3W6fDMHneC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "forward_propagation(image_path1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv7JKrvEYT9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaWHpypjRjSL",
        "colab_type": "code",
        "outputId": "b22f0c23-f8c1-4d3b-8464-33af7abd650a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import numpy as np\n",
        "from numba import cuda, vectorize\n",
        "import scipy.signal as sps\n",
        "\n",
        "input = np.array(\n",
        "[\n",
        " [[124, 148, 167, 174],\n",
        "  [145, 148, 147, 162],\n",
        "  [125, 148, 169, 185],\n",
        "  [131, 163, 185, 191]\n",
        " ],\n",
        "\n",
        " [[157, 181, 199, 203],\n",
        "  [177, 177, 176, 189],\n",
        "  [151, 174, 192, 208],\n",
        "  [150, 182, 204, 210]\n",
        " ],\n",
        "\n",
        " [[110, 134, 152, 159],\n",
        "  [128, 129, 130, 144],\n",
        "  [104, 127, 146, 162],\n",
        "  [105, 137, 159, 165]\n",
        " ]\n",
        "] )\n",
        "\n",
        "# layers, num filters, channels, height, width\n",
        "all_filters = np.array([\n",
        "    [\n",
        "        [\n",
        "            [\n",
        "                [0.1, -0.1, 0.2],\n",
        "                [0.3, 0.2, 0.1],\n",
        "                [-0.1, -0.1, -0.1]\n",
        "            ],\n",
        "            [\n",
        "                [0.2, -0.1, 0.2],\n",
        "                [0.1, 0.3, 0.1],\n",
        "                [-0.2, -0.1, -0.1]\n",
        "            ],\n",
        "            [\n",
        "                [0.1, -0.1, 0.2],\n",
        "                [0.3, 0.2, 0.1],\n",
        "                [-0.1, -0.1, -0.1]\n",
        "            ]\n",
        "        ],\n",
        "        [\n",
        "            [\n",
        "                [0.2, -0.1, 0.2],\n",
        "                [0.1, 0.3, 0.1],\n",
        "                [-0.2, -0.1, -0.1]\n",
        "            ],\n",
        "            [\n",
        "                [0.1, -0.1, 0.2],\n",
        "                [0.3, 0.2, 0.1],\n",
        "                [-0.1, -0.1, -0.1]\n",
        "            ],\n",
        "            [\n",
        "                [0.1, -0.1, 0.2],\n",
        "                [0.3, 0.2, 0.1],\n",
        "                [-0.1, -0.1, -0.1]\n",
        "            ]\n",
        "        ],\n",
        "        [\n",
        "            [\n",
        "                [0.1, -0.1, 0.2],\n",
        "                [0.3, 0.2, 0.1],\n",
        "                [-0.1, -0.1, -0.1]\n",
        "            ],\n",
        "            [\n",
        "                [0.1, -0.1, 0.2],\n",
        "                [0.3, 0.2, 0.1],\n",
        "                [-0.1, -0.1, -0.1]\n",
        "            ],\n",
        "            [\n",
        "                [0.2, -0.1, 0.2],\n",
        "                [0.1, 0.3, 0.1],\n",
        "                [-0.2, -0.1, -0.1]\n",
        "            ]\n",
        "        ]\n",
        "    ],\n",
        "    [\n",
        "        [\n",
        "            [\n",
        "                [0.2, -0.1, 0.2],\n",
        "                [0.1, 0.5, 0.1],\n",
        "                [-0.1, -0.1, -0.1]\n",
        "            ],\n",
        "            [\n",
        "                [0.1, -0.1, 0.2],\n",
        "                [0.3, 0.2, 0.1],\n",
        "                [-0.1, -0.1, -0.1]\n",
        "            ],\n",
        "            [\n",
        "                [0.3, -0.1, 0.2],\n",
        "                [0.3, 0.4, 0.1],\n",
        "                [-0.2, -0.1, -0.1]\n",
        "            ]\n",
        "        ],\n",
        "        [\n",
        "            [\n",
        "                [0.1, -0.1, 0.2],\n",
        "                [0.3, 0.2, 0.1],\n",
        "                [-0.1, -0.1, -0.1]\n",
        "            ],\n",
        "            [\n",
        "                [0.2, -0.1, 0.2],\n",
        "                [0.1, 0.5, 0.1],\n",
        "                [-0.1, -0.1, -0.1]\n",
        "            ],\n",
        "            [\n",
        "                [0.3, -0.1, 0.2],\n",
        "                [0.3, 0.4, 0.1],\n",
        "                [-0.2, -0.1, -0.1]\n",
        "            ]\n",
        "        ],\n",
        "        [\n",
        "            [\n",
        "                [0.1, -0.1, 0.2],\n",
        "                [0.3, 0.2, 0.1],\n",
        "                [-0.1, -0.1, -0.1]\n",
        "            ],\n",
        "            [\n",
        "                [0.3, -0.1, 0.2],\n",
        "                [0.3, 0.4, 0.1],\n",
        "                [-0.2, -0.1, -0.1]\n",
        "            ],\n",
        "            [\n",
        "                [0.2, -0.1, 0.2],\n",
        "                [0.1, 0.5, 0.1],\n",
        "                [-0.1, -0.1, -0.1]\n",
        "            ]\n",
        "        ]\n",
        "    ]\n",
        "])\n",
        "all_biases = np.array([\n",
        "    [\n",
        "        1, 2, 3\n",
        "    ],\n",
        "    [\n",
        "        4, 5, 6\n",
        "    ]\n",
        "])\n",
        "\n",
        "channel = 2\n",
        "filters = all_filters[0, 2, channel, :, :]\n",
        "cor_input = np.pad(input[channel], (1, 1), mode='constant')[:5, :5]\n",
        "\n",
        "\n",
        "a = sps.correlate(cor_input, filters, mode='valid')\n",
        "\n",
        "print(a)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[20.7 14.9 21.7]\n",
            " [44.  55.4 53.5]\n",
            " [32.7 51.2 54.5]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}